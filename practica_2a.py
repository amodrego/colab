# -*- coding: utf-8 -*-
"""Practica_2a.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PlMKw9zN1W9DgYJmxqh_QWzD3mUDlMAj

# Práctica 2a: Redes neuronales

En esta práctica se introduce el concepto de red neuronal de tipo perceptrón multicapa y de
tipo convolucional. El trabajo se realiza en Python sobre un toolkit de cálculo automático de
gradientes tensorflow + keras.

Importamos librerías y datos necesarios
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
# %matplotlib inline
import matplotlib.pyplot as plt
from scipy import stats
import pickle, gzip, matplotlib 
import matplotlib.pyplot as plt

# Imports de tensorflow y keras
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, activations

# Descargar datos
!wget https://dihana.cps.unizar.es/~cadrete/mlbio/p2_data1.pkl.gz --no-check-certificate

"""## Funciones Apéndice
(Necesarias más adelante)
"""

def view_data1(x, y):
    k = 0
    for i in range(4):
        for j in range(4):
            plt.subplot(4, 4, k+1)
            plt.imshow( x[k].reshape(32, 32), interpolation='none', cmap='gray')
            plt.title(r'$y_{%d}$ = %d' % (k, y[k]))
            plt.gca().axis('off')
            k += 1
    plt.show()

def view_data2(x, y):
    k = 0
    for i in range(2):
        for j in range(2):
            plt.subplot(2, 4, 2*k+1)
            plt.imshow( x[k].reshape(32, 32), interpolation='none', cmap='gray')
            plt.title(r'$x_{%d}$' % k)
            plt.gca().axis('off')
            
            plt.subplot(2, 4, 2*k+2)
            plt.imshow( y[k].reshape(32, 32), interpolation='none',cmap='gray')
            plt.title(r'$y_{%d}$' % k)
            plt.gca().axis('off')
            k += 1
    plt.show()

def view_data3(x, y, o):
    k = 0
    for i in range(2):
        for j in range(2):
            
            plt.subplot(2, 6, 3*k+1)
            plt.imshow( x[k].reshape(32, 32), interpolation='none', cmap='gray')
            plt.title(r'$x_{%d}$' % k)
            plt.gca().axis('off')
        
            plt.subplot(2, 6, 3*k+2)
            plt.imshow( y[k].reshape(32, 32), interpolation='none',cmap='gray')
            plt.title(r'$y_{%d}$' % k)
            plt.gca().axis('off')

            plt.subplot(2, 6, 3*k+3)
            plt.imshow( o[k].reshape(32, 32), interpolation='none',cmap='gray')
            plt.title(r'$o_{%d}$' % k)
            plt.gca().axis('off')
            k += 1
    plt.show()

"""## 1. Problema de clasificación global

### 1.1 Red MLP

El primer problema se trata de una tarea de clasificación de imagen de tipo global, es decir a
cada imagen completa le corresponde una categoría de clase. Por lo tanto, el objetivo en el diseño
de la arquitectura es dar como salida un vector de tantas dimensiones como clases.

Primero se extraen los datos que hemos descargado previamente en las variables de entrenamiento y de test
"""

with gzip.open('p2_data1.pkl.gz', 'rb') as f:
 x_train, y_train, x_test, y_test = pickle.load(f)

"""Normalizamos los valores, que van de rango 0-255

"""

x_train = x_train / 255
x_test = x_test / 255
view_data1(x_train, y_train)

"""Para definir una red neuronal en keras lo mas facil es utilizar la funcion Sequential.
Esta es una lista de módulos que se aplicarán a los datos de forma sucesiva en
el orden en el que se definen.

Es una lista por lo que se declara mediante|| y se separan los elementos por comas

A modo de ejemplo, se crea un red MLP con 100 dimensiones de entrada y 10 de salida
"""

model1 = keras.Sequential([
          keras.Input(shape=(100,)),
          layers.Dense(128),
          layers.Activation(activations.tanh),
          layers.Dense(10),
          layers.Softmax()
          ])

"""El primer tipo de sistema que probamos en la práctica va a ser de una red de tipo MLP en
keras. Como las dimensiones de estos datos de imagen son 32 x 32 x 1, para usar una red de tipo
MLP lo primero que haremos es cambiar el tamaño para que sea compatible con este tipo de red,
es decir tenemos que pasar a vector. De esa forma nos quedarán como vectores de 322
= 1024
"""

model1 = keras.Sequential([
          keras.Input(shape=(32, 32, 1)),
          layers.Reshape( (1024,)),
          layers.Dense(128),
          layers.Activation(activations.tanh),
          layers.Dense(2),
          layers.Softmax(),
          ])

"""Informacion de la red

"""

print( model1.summary() ) # Resumen de la red
keras.utils.plot_model(model1, 'model1.png', show_shapes=True)  # Visualizar grafo

"""Compilamos y definimos la función de coste utilizando como optimizador el SGD"""

model1.compile(
 loss=keras.losses.SparseCategoricalCrossentropy(),
 optimizer=keras.optimizers.SGD(learning_rate=0.1, momentum=0.8),
 metrics=[keras.metrics.SparseCategoricalAccuracy()])

"""Entrenamos la red"""

log = model1.fit(x_train, y_train, batch_size=32, epochs=50, validation_split=0.15)

"""Podemos además representar informacion útil del entrenamiento almacenada"""

plt.clf()
plt.plot(log.history['loss'], 'b')
plt.plot(log.history['val_loss'], 'r')
plt.grid(True)
plt.gca().legend(['train','dev'], loc='upper right',fontsize=8)

"""c)

Para medir el error de los modelos que vamos a entrenar lo haremos de la siguiente manera
"""

y_pred_test = model1.predict(x_test)
err = np.sum( y_pred_test.argmax(1) != y_test )
print('error rate test: %f %%' % (err / len(y_test) * 100))

model1.predict(x_test)

"""### 1.2. Red Convolucional

A continuación, vamos a tratar de resolver el mismo problema mediante una red
convolucional. Lo más habitual es utilizar uno de estos dos estilos de diseño para obtener una
salida global usando redes convolucionales: mediante vectorización de la imagen después de varias
etapas convolucionales (reshape / flatten) la llamaremos tipo 1, o mediante una reducción global
(pooling), tipo 2

Las redes que diseñaremos tienen dos partes. La primera parte puede tener un diseño
similar sea cual sea el tipo de conversión a vector, combina capas convolucionales y no
linealidades y se suele ir reduciendo el tamaño de los datos (diezmado: stride > 1) conforme
avanzamos en profundidad. Además, conforme aumenta la profundidad y se reduce la
información espacial, va aumentado el número de canales
"""

model1 = keras.Sequential([
 keras.Input(shape=(32, 32, 1)),
 layers.Conv2D(16, kernel_size=(5, 5), strides=(2, 2),
 activation='relu', padding='same'),
 layers.Conv2D(32, kernel_size=(3, 3), strides=(2, 2),
 activation='relu', padding='same'),
 layers.Conv2D(64, kernel_size=(3, 3), strides=(2, 2),
 activation='relu', padding='same')
])

print(model1.predict(x_test).shape)

"""**1.2**.1. Red Convolucional tipo 1

La salida de la red será como en el caso del MLP de la red anterior un vector del tamaño del
número de clases, en este caso 2. Para conseguirlo tenemos que aplicar en algún momento una
operación que transforme las señales con formato de imagen con varios canales, es decir, con
dimensiones: (N x H x W x C), con N el número de ejemplos del batch, H y W el tamaño de la
imagen y C los canales al tamaño (N x D) donde de D es la dimensión en forma de vector.

En primer lugar se ve el ejemplo de serialización de la información, es decir, si leemos por orden todos los elementos
de ese tensor podríamos convertir un tensor como el del ejemplo (N x H x W x C) a una matriz
de tamaño (N x D) donde D = H*W*C. Esto se hace mediante la función Reshape, aunque podríamos utilzar también Flatten
"""

model1 = keras.Sequential([
  keras.Input(shape=(32, 32, 1)),
  layers.Conv2D(16, kernel_size=(5, 5), strides=(2, 2),
  activation='relu', padding='same'),
  layers.Conv2D(32, kernel_size=(3, 3), strides=(2, 2),
  activation='relu', padding='same'),
  layers.Conv2D(64, kernel_size=(3, 3), strides=(2, 2),
  activation='relu', padding='same'),
  layers.Reshape( (4 * 4 * 64,)),
  layers.Dense(128),
  layers.Activation(activations.tanh),
  layers.Dense(2),
  layers.Softmax(),
])

"""Realizamos el mismo proceso que antes para entrenar la red y obtener los datos importantes de este entrenamiento.


"""

model1.compile(
 loss=keras.losses.SparseCategoricalCrossentropy(),
 optimizer=keras.optimizers.SGD(learning_rate=0.1, momentum=0.8),
 metrics=[keras.metrics.SparseCategoricalAccuracy()])

log = model1.fit(x_train, y_train, batch_size=32, epochs=50, validation_split=0.15)

# Representacion del entrenamiento de la red de forma gráfica
plt.clf()
plt.plot(log.history['loss'], 'b')
plt.plot(log.history['val_loss'], 'r')
plt.grid(True)
plt.gca().legend(['train','dev'], loc='upper right',fontsize=8)

y_pred_test = model1.predict(x_test)
err = np.sum( y_pred_test.argmax(1) != y_test )
print('error rate test: %f %%' % (err / len(y_test) * 100))

model1.predict(x_test)

"""1.2.2. Red Convolucional tipo 2

La otra técnica para reducir el tamaño al deseado es utilizar lo que se llama una operación de
reducción o Pooling como la media, el máximo, etc., sobre las dimensiones espaciales y/o
temporales según sea el tipo de dato de interés. En el caso de nuestra imagen son las dimensiones
H y W, por tanto, utilizaremos un Pooling2d. La operación da como resultado una matriz de
tamaño (N x C).

El procedimiento es exactamente igual que los anteriores, solo que añadiendo dentro de las capas de la red este Pooling.
Esta forma de reducir las dimensiones espaciales es especialmente útil si queremos que el sistema
funcione incluso con imágenes de entrada de distintos tamaños, ya que no le afecta el tamaño de
entrada
"""

model1 = keras.Sequential([
  keras.Input(shape=(32, 32, 1)),
  layers.Conv2D(16, kernel_size=(5, 5), strides=(2, 2),
  activation='relu', padding='same'),
  layers.Conv2D(32, kernel_size=(3, 3), strides=(2, 2),
  activation='relu', padding='same'),
  layers.Conv2D(64, kernel_size=(3, 3), strides=(2, 2),
  activation='relu', padding='same'),
  layers.GlobalAveragePooling2D(),
  layers.Dense(128),
  layers.Activation(activations.tanh),
  layers.Dense(2),
  layers.Softmax(),
])

model1.compile(
 loss=keras.losses.SparseCategoricalCrossentropy(),
 optimizer=keras.optimizers.SGD(learning_rate=0.1, momentum=0.8),
 metrics=[keras.metrics.SparseCategoricalAccuracy()])

# Compilar y entrenar la red
log = model1.fit(x_train, y_train, batch_size=32, epochs=50, validation_split=0.15)

# Representacion del entrenamiento de la red de forma gráfica
plt.clf()
plt.plot(log.history['loss'], 'b')
plt.plot(log.history['val_loss'], 'r')
plt.grid(True)
plt.gca().legend(['train','dev'], loc='upper right',fontsize=8)

y_pred_test = model1.predict(x_test)
err = np.sum( y_pred_test.argmax(1) != y_test )
print('error rate test: %f %%' % (err / len(y_test) * 100))

model1.predict(x_test)

"""Práctica con GlobalPooling y Reshape"""

!wget https://dihana.cps.unizar.es/~cadrete/mlbio/p2_data1_biased.pkl.gz --no-check-certificate
with gzip.open('p2_data1_biased.pkl.gz', 'rb') as f:
 x_train, y_train, x_test, y_test = pickle.load(f)
 
#  Normalizamos los valores, que van de rango 0-255
x_train = x_train / 255
x_test = x_test / 255

"""Se observa que las imágenes están todas movidas hacia la derecha, lo que puede suponer un problema a la hora de entrenar a la red y que sea capaz de diferenciar los números cuando estos se encuentran a la izquierda de la imagen."""

view_data1(x_train, y_train)

"""Reshape"""

# Se vuelve a entrenar la red esta vez con los datos nuevos

model1 = keras.Sequential([
  keras.Input(shape=(32, 32, 1)),
  layers.Conv2D(16, kernel_size=(5, 5), strides=(2, 2),
  activation='relu', padding='same'),
  layers.Conv2D(32, kernel_size=(3, 3), strides=(2, 2),
  activation='relu', padding='same'),
  layers.Conv2D(64, kernel_size=(3, 3), strides=(2, 2),
  activation='relu', padding='same'),
  layers.Reshape( (4 * 4 * 64,)),
  layers.Dense(128),
  layers.Activation(activations.tanh),
  layers.Dense(2),
  layers.Softmax(),
])
# Compilar y entrenar la red
model1.compile(
 loss=keras.losses.SparseCategoricalCrossentropy(),
 optimizer=keras.optimizers.SGD(learning_rate=0.1, momentum=0.8),
 metrics=[keras.metrics.SparseCategoricalAccuracy()])

log = model1.fit(x_train, y_train, batch_size=32, epochs=50, validation_split=0.15)

"""Representacion del modelo gráficamente junto con el valor del error

"""

plt.clf()
plt.plot(log.history['loss'], 'b')
plt.plot(log.history['val_loss'], 'r')
plt.grid(True)
plt.gca().legend(['train','dev'], loc='upper right',fontsize=8)

y_pred_test = model1.predict(x_test)
err = np.sum( y_pred_test.argmax(1) != y_test )
print('error rate test: %f %%' % (err / len(y_test) * 100))

model1.predict(x_test)

"""Se visualizan los datos ya tratados mediante la red

"""

view_data1(x_test, y_test)

"""Pooling"""

model1 = keras.Sequential([
  keras.Input(shape=(32, 32, 1)),
  layers.Conv2D(16, kernel_size=(5, 5), strides=(2, 2),
  activation='relu', padding='same'),
  layers.Conv2D(32, kernel_size=(3, 3), strides=(2, 2),
  activation='relu', padding='same'),
  layers.Conv2D(64, kernel_size=(3, 3), strides=(2, 2),
  activation='relu', padding='same'),
  layers.GlobalAveragePooling2D(),
  layers.Dense(128),
  layers.Activation(activations.tanh),
  layers.Dense(2),
  layers.Softmax(),
])

model1.compile(
 loss=keras.losses.SparseCategoricalCrossentropy(),
 optimizer=keras.optimizers.SGD(learning_rate=0.1, momentum=0.8),
 metrics=[keras.metrics.SparseCategoricalAccuracy()])

log = model1.fit(x_train, y_train, batch_size=32, epochs=50, validation_split=0.15)

"""Representacion del modelo gráficamente junto con el valor del error

"""

plt.clf()
plt.plot(log.history['loss'], 'b')
plt.plot(log.history['val_loss'], 'r')
plt.grid(True)
plt.gca().legend(['train','dev'], loc='upper right',fontsize=8)

y_pred_test = model1.predict(x_test)
err = np.sum( y_pred_test.argmax(1) != y_test )
print('error rate test: %f %%' % (err / len(y_test) * 100))

model1.predict(x_test)

"""Se obtiene un porcentaje de error mejor en la red creada mediante Pooling. Esto es debido a que, si se modifican las dimensiones mediante Reshape, la red se entrena bien, sin embargo, al hacer el test y pasarle numeros a la izquierda de la imagen, esta no sabe diferenciar tan bien como lo hace mediante Pooling, ya que en esta última generaliza más y no se fija tanto en la posición exacta del número.

## 2. Problema de Regresión multiple

El segundo problema al que nos enfrentamos es una regresión múltiple. Lo resolveremos
mediante la minimización del error cuadrático (MSE). Vamos a realzar la imagen de entrada que
tiene ruido para que se parezca pixel a pixel a una de referencia limpia. Este tipo de problema lo
podemos ver hoy en día en muchas aplicaciones en las que se realza una imagen deteriorada (por
ruido, desenfoque, …), se añade color a una imagen de blanco y negro o se incrementa la
resolución de la imagen (superresolución)

Primero se leen los datos y se normalizan para trabajar con ellos
"""

!wget https://dihana.cps.unizar.es/~cadrete/mlbio/p2_data2.pkl.gz --no-check-certificate
with gzip.open('p2_data2.pkl.gz', 'rb') as f:
 x_train, y_train, x_test, y_test = pickle.load(f)
x_train = x_train / 255
x_test = x_test / 255
y_train = y_train / 255
y_test = y_test / 255

"""a) Leer datos y comprobar tamaños de tensores

Mediante la función de apéndice view_data podemos visualizar las imágenes
"""

view_data2(x_train, y_train)

# Datos de test
view_data2(x_test, y_test)

"""Se genera un modelo con el que se decrece la imagen hasta poder compararla con los datos que tenemos, que son de 32x32x1"""

model1_clean = keras.Sequential([
 keras.Input(shape=(32, 32, 1)),
 layers.Conv2D(16, kernel_size=(3, 3),
 activation='relu', padding='same'),
#  layers.Conv2D(32, kernel_size=(3, 3),
#  activation='relu', padding='same'),
 layers.Conv2D(16, kernel_size=(3, 3),
 activation='relu', padding='same'),
 layers.Conv2D(1, kernel_size=(3, 3),
 activation='relu', padding='same'),
])

model1_clean.summary()

"""Entrenar y ver log de la red

"""

model1_clean.compile(
 loss=keras.losses.MeanSquaredError(),
 optimizer=keras.optimizers.SGD(learning_rate=0.1, momentum=0.8),
 metrics=[keras.metrics.SparseCategoricalAccuracy()])

log = model1_clean.fit(x_train, y_train, batch_size=32, epochs=20, validation_split=0.15)

plt.clf()
plt.plot(log.history['loss'], 'b')
plt.plot(log.history['val_loss'], 'r')
plt.grid(True)
plt.gca().legend(['train','dev'], loc='upper right',fontsize=8)

out_test = model1_clean.predict(x_test)
view_data3(x_test, y_test, out_test)